# BDL Assignment 5: Tracking Model Building Experiments with MLFlow

This repository contains the code and instructions for the BDL (Big Data and Learning) Assignment 5, which focuses on tracking model building experiments using the MLFlow platform.
Description
The assignment aims to build several neural network models for MNIST digit classification with different model configuration settings. Each configuration setting will show different patterns in model building performance measurements over every learning epoch. The goal is to compare the model performance trends and understand the impact of model configuration settings.
Instead of manually coding for data collection and visualization, MLFlow is used to automatically track the metrics and parameters during the model building process.
Files

A05-Tracking a Model building experiment in MLFlow.pdf: This file contains the assignment instructions and tasks.
MM20B007 BDL Assignment 5.pdf: This file contains the Jupyter Notebook code for building the classification models with different parameter settings and logging the experiments using MLFlow.

Tasks
The main tasks of the assignment are:

Modify the code to log metrics (loss, accuracy) using mlflow.autolog() and mlflow.log_metrics().
Modify the code to log parameters (network configuration, learning rate, optimizer, regularization, etc.) using mlflow.autolog() and mlflow.log_param().
Use the with mlflow.start_run() construct to run the model build for each configuration, creating a new entry in MLflow for each run.
Configure the entire MNIST experiment as an experiment, with each neural configuration becoming a sub-experiment.
Visualize and compare the performance numbers using the "Compare" button in the MLflow UI, taking snapshots of the comparison plots.
Compare models across experiments and take snapshots of the plots.

Requirements

Python
TensorFlow
MLFlow

Usage

Clone the repository
Install the required dependencies
Navigate to the project directory
Run the Jupyter Notebook MM20B007 BDL Assignment 5.pdf
Follow the instructions in the notebook and the assignment file to complete the tasks
View and analyze the logged experiments in the MLflow UI

Submission
The submission for this assignment should include:

The completed Jupyter Notebook file
Snapshots of the MLflow console, showing the logged experiments and performance comparisons

Contributing
Contributions to this repository are welcome. If you find any issues or have suggestions for improvement, please open an issue or submit a pull request.
